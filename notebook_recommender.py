# -*- coding: utf-8 -*-
"""notebook_recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VNU9QiXHElFLIeq0WnAtrG-qELQAR1VW

# **Recommendation System Project: Sistem Rekomendasi Anime dengan Metode Content-based Filtering dan Collaborative Filtering**
- **Nama: Diva Anggreini Harahap**
- **Email: divaanggreiniharahap@gmail.com**
- **ID Dicoding: divaaharahap / MC319D5X2329**

## **About Dataset**
- Dataset didapat dari Kaggle.
- Link : https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database
- Dataset ini berisi preferensi pengguna terhadap anime, yang diambil dari MyAnimeList.net
- Terdiri dari dua file utama:
<br>
anime.csv - Metadata Anime :
- anime_id :	ID unik anime dari MyAnimeList
- name : Judul lengkap anime
- genre : Daftar genre (dipisah koma)
- type : Jenis tayangan (TV, Movie, OVA, dll)
- episodes : Jumlah episode (1 jika film)
rating : Rata-rata rating komunitas (out of 10)
- members :	Jumlah member yang memiliki anime ini di daftar mereka
<br>
rating.csv - interaksi pengguna
- user_id : id acak pengguna
- anime_id : id anime yang dirating
- rating : rating pengguna
"""

!pip install tensorflow gdown

# import library yang diperlukan
import gdown
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import os
import warnings
warnings.filterwarnings('ignore')
from keras import layers

"""# **Data Understanding**"""

anime = '15EcirxeK8mmBf5uOMZqFPNEK8zC5wF5O'
ratings = '1t8eav9wJ3dmofVopD2w71XiN01mMCL7n'

gdown.download(f'https://drive.google.com/uc?id={anime}', 'anime.csv', quiet=False)
gdown.download(f'https://drive.google.com/uc?id={ratings}', 'ratings.csv', quiet=False)

# load csv
anime = pd.read_csv('anime.csv')
ratings = pd.read_csv('ratings.csv')

anime.head(5)

ratings.head(5)

# melihat jumlah baris dan kolom
anime.shape

ratings.shape

"""- Dataset `anime.csv` memiliki 12.294 baris, yang berarti terdapat 12.294 anime unik dan memiliki 7 kolom fitur.
- Dataset `rating.csv` memiliki 7.813.737 baris, artinya ada hampir 8 juta interaksi user terhadap anime dan memiliki 3 kolom fitur
"""

# memahami tipe data tiap kolom
anime.info()

"""Kolom yang bertipe numerik
- anime_id
- rating
- members

Kolom yang bertipe object
- name
- genre
- type
- episodes
"""

ratings.info()

"""Semua kolom bertipe numerik.

# **Univariate Exploratory Data Ananlysis**
"""

# analisis statistik deskriptif (kolom numerik)
anime.describe()

"""**Kesimpulan**
- Banyak anime yang belum dirating (rating<count)
- Rating komunitas rata-rata tinggi
- Penyebearan member sangat timpang
"""

# Filter data valid
valid_ratings = ratings[ratings['rating'] != -1]

# Plot distribusi
plt.figure(figsize=(8, 5))
sns.histplot(valid_ratings['rating'], bins=10, kde=True)
plt.title('Distribusi Rating User')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.grid(True)
plt.show()

"""**Insight**
- User cenderung memberi rating tinggi, sehingga sistem harus hati-hati agar tidak menganggap semua anime bagus hanya karena rating tinggi.
"""

# anime dengan member terbanyak
top_popular = anime[['name', 'members']].sort_values(by='members', ascending=False).head(10)
print("Anime Paling Populer:\n", top_popular)

top_popular.head(10).plot(kind='barh', x='name', y='members', figsize=(8,5), color='skyblue')
plt.title('Top 10 Anime dengan Member Terbanyak')
plt.xlabel('Jumlah Member')
plt.ylabel('Anime')
plt.gca().invert_yaxis()
plt.show()

ratings.describe()

"""Data cenderung positif yang berarti user lebih sering memberi rating tinggi.

# **Data Preprocessing**

mengubah nama kolom rating di anime.csv biar gak duplikat dengan kolom rating di rating.csv.
"""

# ubah nama kolom
anime.rename(columns={'rating': 'mean_rating'}, inplace=True)

# menggabungkan dataset ratings dan movies bedasarkan movieId
merged_df = ratings.merge(anime, on='anime_id', how='left')

merged_df

# cek missing values
merged_df.isnull().sum()

"""Ada banyak missing values di kolom name,genre, type, episodes, mean_rating dan members. Nanti akan kita tanangi dengan imputasi atau penghapusan."""

# Cek duplikat
print(f"Jumlah duplikat: {merged_df.duplicated().sum()}")

print(f"Duplikat berdasarkan user_id dan name: {merged_df.duplicated(subset=['user_id', 'name']).sum()}")

"""# **Data Preparation**"""

# handling data duplikat
merged_df = merged_df.drop_duplicates(subset=['user_id', 'name'], keep='first')
print("Duplikat tersisa:", merged_df.duplicated(subset=['user_id', 'name']).sum())

# handling missing values
merged_df = merged_df.dropna(subset=['name'])

# Isi nilai kosong kolom genre, type
merged_df['genre'] = merged_df['genre'].fillna('Unknown')
merged_df['type'] = merged_df['type'].fillna('Unknown')

# hapus baris yang kolom episodesnya unknown
merged_df = merged_df[merged_df['episodes'] != 'Unknown']

# isi mean_rating dengan nilai mean
merged_df['mean_rating'] = merged_df['mean_rating'].fillna(merged_df['mean_rating'].mean())

# isi members dengan nilai yang paling sering muncul
merged_df['members'] = merged_df['members'].fillna(merged_df['members'].median())

print(f"Total missing values: {merged_df.isnull().sum().sum()}")

merged_df[merged_df.isnull().any(axis=1)]

# Konversi sisa kolom menjadi float
merged_df['episodes'] = merged_df['episodes'].astype(float)

# gabung kolom untuk content
merged_df['content'] = merged_df['type'] + " " + merged_df['genre']

# filter kolom yang tidak diperlukan
df_model = merged_df[['name', 'episodes', 'content']]

df_model = df_model.drop_duplicates(subset='name', keep='first')

"""Menghapus data duplikat berdasarkan kolom `name`. Jika ada duplikat, baris pertama akan dipertahankan sisanya dihapus."""

df_model

"""# **Model Development dengan Content Based Filtering**"""

# mengubah content jadi vektor numerik dengan TF-IDF
tfidf = TfidfVectorizer()

tfidf.fit(df_model['content'])

#mapping array dari fitur index integer ke fitur nama
tfidf.get_feature_names_out()

# melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf.fit_transform(df_model['content'])
tfidf_matrix.shape

"""Ini akan mengubah teks menjadi representasi numerik menggunakan TF-IDF Vectorizer, menunjukkan ada 11.196 baris dan setiap anime direpresentasikan oleh 53 fitur unik yang diekstrak dari kolom content."""

# mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""`.todense()` digunakan untuk mengubah sparse matrix ke bentuk penuh yang berisi semua angka, nantinya ini akan digunakan untuk melihat isi matrix atau dipakai di fungsi yang butuh dense input"""

tfidf_df = pd.DataFrame(
    tfidf_matrix.todense().round(2),
    columns=tfidf.get_feature_names_out(),
    index=df_model['name']
)

# Menampilkan 10 lagu dan 10 fitur secara acak
tfidf_df.sample(10, axis=0).sample(10, axis=1)

"""Output diatas menunjukkan seberapa penting setiap kata (genre dan tipe) bagi masing-masing anime berdasaran perhitungan TF-IDF. Nilainya menunjukkan relevansi kata terhadap konten anime. Data ini digunakan untuk mengukur kemiripan antar anime."""

# menghitung kemiripan antar lagu dengan cosine similarity
cosine_sim = cosine_similarity(tfidf_matrix)

# membuat Series mapping judul lagu ke index
indices = pd.Series(df_model.index, index=df_model['name'])

def recommend(title, top_n=10):
    if title not in indices:
        return f"Produk dengan judul '{title}' tidak ditemukan."

    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))

    # urutkan berdasarkan skor similarity tertinggi dan ambil top_n (kecuali anime itu sendiri)
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]

    product_indices = [i[0] for i in sim_scores]
    scores = [i[1] for i in sim_scores]

    results = df_model.iloc[product_indices][['name', 'episodes']].copy()
    results['Similarity_Score'] = scores
    results['Similarity_Score'] = results['Similarity_Score'].round(2)

    return results

recommend('Naruto')

recommend('Wolverine')

# evaluasi dengan precision@N
relevant_titles = [
    "Naruto x UT",
    "Naruto: Shippuuden Movie 3 - Hi no Ishi wo Tsugu Mono",
    "Naruto: Shippuuden Movie 4 - The Lost Tower",
    "Naruto Soyokazeden Movie: Naruto to Mashin to...",
    "Boruto: Naruto the Movie",
    "Rekka no Honoo",
    "Dragon Ball Z"
]

def evaluate_precision(recommendations, relevant_titles):
    recommended_titles = recommendations['name'].tolist()
    relevant_hits = [title for title in recommended_titles if title in relevant_titles]
    precision = len(relevant_hits) / len(recommended_titles)
    return round(precision, 2)

results = recommend("Naruto", top_n=10)
precision = evaluate_precision(results, relevant_titles)
print(f"Precision@10: {precision}")

"""Dari 10 anime yang direkomendasikan, sebanyak 6 anime yang memang relevan atau cocok dengan preferensi yang diharapkan.

Dengan precision@10 = 0.6, akurasi rekomendasi sudah cukup baik. Karena setiap kali mendekati 1.0, semakin bagus, ini menunjukkan sistem ini sudah mulai bisa mengenali pola kesamaan konten dengan baik.

# **Model Development dengan Collaborative Filtering**
"""

cf_df = merged_df.copy()

cf_df

"""## Data Preparation"""

# cek semua rating
cf_df[cf_df['rating'] < 0]

# drop rating yang dibawah 1
cf_df = cf_df[cf_df['rating'] >= 1]

user_ids = cf_df['user_id'].unique().tolist()
print('list user_id : ', user_ids)

# encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id: ', user_to_user_encoded)

# melakukan proses encoding angka ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

anime_ids = cf_df['anime_id'].unique().tolist()
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

# mapping user_id ke df user
cf_df['user'] = cf_df['user_id'].map(user_to_user_encoded)

# mapping anime_id ke df anime
cf_df['anime'] = cf_df['anime_id'].map(anime_to_anime_encoded)

# mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# mendapatkan jumlah anime
num_anime = len(anime_to_anime_encoded)
print(num_anime)

# mengubah rating menjadi float
cf_df['rating'] = cf_df['rating'].astype(float)

# nilai minimum rating
min_rating = min(cf_df['rating'])
print(min_rating)

# nilai maksimum rating
max_rating = max(cf_df['rating'])
print(max_rating)

# membagi data untuk training dan validasi
cf_df = cf_df.sample(frac=1, random_state=42)
cf_df

# membuat variabel x untuk mencocokkan data user dan anime menjadi satu value
x = cf_df[['user', 'anime']].values

# membuat variabel y untuk membuat rating dari hasil
y = cf_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * cf_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x,y)

"""## Proses Training"""

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_anime, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.anime_embedding = layers.Embedding(
            input_dim=num_anime,
            output_dim=embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.anime_bias = layers.Embedding(num_anime, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        anime_vector = self.anime_embedding(inputs[:, 1])
        anime_bias = self.anime_bias(inputs[:, 1])

        dot_user_anime = tf.reduce_sum(user_vector * anime_vector, axis=1, keepdims=True)
        x = dot_user_anime + user_bias + anime_bias
        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_anime, 50)

callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_root_mean_squared_error',
    factor=0.5,
    patience=2,
    verbose=1
)

model.compile(
    loss=tf.keras.losses.Huber(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x_train, y_train,
    batch_size=2048,
    epochs=10,
    validation_data=(x_val, y_val),
    callbacks=[callback],
    verbose=1
)

# visualisasi metrik
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Root Mean Squared Error over Epochs')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)
plt.show()

"""- Model berhasil belajar dengan baik dan konsisten, karena error di data training dan validasi menurun seiring waktu.
- Tidak ada overfitting, karena RMSE validasi tidak naik ketika training terus dilanjutkan.
- Nilai RMSE akhir (~0.133) sangat baik untuk sistem rekomendasi berbasis rating skala 0–1 → artinya prediksi model cukup akurat (error rata-rata sangat kecil).
"""

def recommend(user_id_raw, model, cf_df, anime_encoded_to_anime, anime_id_to_title=None, top_n=10):
    # Encode user_id
    user_encoded = user_to_user_encoded.get(user_id_raw)
    if user_encoded is None:
        return f"User ID {user_id_raw} tidak ditemukan dalam data"

    anime_rated = cf_df[cf_df['user'] == user_encoded]['anime'].tolist()

    all_anime = np.arange(num_anime)
    anime_to_predict = [i for i in all_anime if i not in anime_rated]

    user_array = np.full(len(anime_to_predict), user_encoded)
    input_array = np.stack((user_array, anime_to_predict), axis=1)

    # prediksi rating
    predictions = model.predict(input_array, verbose=0).flatten()

    # mengambil top N
    top_indices = predictions.argsort()[-top_n:][::-1]
    top_anime_encoded = [anime_to_predict[i] for i in top_indices]
    top_predicted_scores = predictions[top_indices]

    # mengembalikan ke anime_id
    top_anime_ids = [anime_encoded_to_anime[i] for i in top_anime_encoded]

    # menampilkan hasil
    recommendations = []
    for anime_id, score in zip(top_anime_ids, top_predicted_scores):
        title = anime_id_to_title.get(anime_id, anime_id) if anime_id_to_title else anime_id
        recommendations.append((title, float(score)))

    return recommendations

# contoh dict judul
anime_id_to_title = dict(zip(anime['anime_id'], anime['name']))

# mendapatkan rekomendasi untuk user 100
recs = recommend(user_id_raw=100, model=model, cf_df=cf_df,
                 anime_encoded_to_anime=anime_encoded_to_anime,
                 anime_id_to_title=anime_id_to_title, top_n=5)

for title, score in recs:
    print(f"{title}: prediksi skor {score:.3f}")

"""Akhirnya, model berhasil memberikan rekomendasi anime untuk user_id 100. Prediksi skor menunjukkan seberapa besar kemungkinan user ini menyukai anime tersebut, berdasarkan model CF ini."""